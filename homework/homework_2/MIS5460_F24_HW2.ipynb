{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Labeling & Peer Grading:** Your homework will be peer graded. To stay anonymous, avoid using your name and label your file with the last four digits of your student ID (e.g., HW2_Solutions_3938).\n",
    "\n",
    "2. **Submission:** Submit both your IPython notebook (.ipynb) and an HTML file of the notebook to Canvas under Assignments → HW 2 → Submit Assignment. After submitting, download and check the files to make sure that you've uploaded the correct versions. Both files are required for your HW to be graded.\n",
    "3. \n",
    " <font color='red'> No pdf file required so write all the details in your ipynb file.</font>\n",
    "2. **AI Use Policy:** Solve each problem independently by yourself. Use AI tools like ChatGPT or Google Gemini for brainstorming and learning only—copying AI-generated content is prohibited. You do not neeViolations will lead to penalties, up to failing the course.\n",
    "\n",
    "3. **Problem Structure:** <font color='red'>Break down each problem ( already done in most problems) into three interconnected parts and implement each in separate code cells. Ensure that each part logically builds on the previous one. Include comments in your code to explain its purpose, followed by a Markdown cell analyzing what was achieved. After completing all parts, add a final Markdown cell reflecting on your overall approach, discussing any challenges faced, and explaining how you utilized AI tools in your process.\n",
    "</font>\n",
    "4. **Deadlines & Academic Integrity:** This homework is due on 10/01/2024 at midnight. <font color='red'>Disclosure of this assignment and assignment answers to anyone or any website is a contributory infringement of academic dishonesty at ISU. Do not share or post course materials without the express written consent of the copyright holder and instructor. The class will follow Iowa State University’s policy on academic dishonesty. Anyone suspected of academic dishonesty will be reported to the Dean of Students Office.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Each problem is worth 20 points. Total $\\bf 20\\times 5 = 100$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1. \n",
    "Upload the newimdb data and read the details on the columns about the data following the link https://www.imdb.com/interfaces/.\n",
    "* Begin the data cleaning process by addressing duplicate entries in the dataset, with a particular focus on the 'category' and 'directors' columns. Remove any redundant rows and determine the best approach for handling variations within these columns. You may choose to either aggregate the diverse values into lists or select the most frequent entry for each instance; explain the reasoning behind your chosen method. Subsequently, identify and eliminate irrelevant columns, such as 'Unnamed: 0', which do not contribute meaningful information for regression or classification tasks. Provide a clear justification for the removal of each column, taking into account their potential significance in model development. Your explanation should demonstrate a thorough understanding of the dataset's structure and the relevance of each feature to the intended analysis.\n",
    "* Analyze the dataset for missing values, particularly in columns like 'runtimeMinutes' and 'genres'. Determine whether to remove rows with missing data or impute values using appropriate statistical methods (mean, median, or mode), and justify your approach. For feature engineering, transform non-numerical columns such as 'genres' and 'directors' using one-hot encoding or label encoding techniques. Consider grouping genres into broader categories to potentially improve model performance. Provide clear explanations for your strategies in handling missing values and implementing feature engineering, ensuring your decisions are data-driven and aligned with the goals of the analysis.\n",
    "  \n",
    "* Conduct exploratory data analysis (EDA) on all remaining columns. Handle non-numerical values appropriately, convert data types as needed, and apply transformations or standardization where necessary. Implement dummy coding for categorical variables. For classification purposes, create a new 'Rating' column derived from 'averageRating'. Instead of using a predetermined scale, devise a binning strategy that results in a balanced class distribution across 5 rating categories (1 to 5 stars). Explain your chosen binning method and analyze the resulting distribution. Discuss how this balanced approach may impact your classification model's performance compared to using the original 'averageRating' values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.  \n",
    "You will make regression models here using the cleaned data from problem 1. Use averageRating as the target variable for this problem.\n",
    "* Begin by selecting five advanced machine learning models for multiple linear/nonlinear regression, excluding basic models like OLS, Decision Tree, Random Forest, K-Nearest Neighbors, and SVM. Determine appropriate test size and validation method, providing a clear rationale for your choices. Split your data into training and testing sets based on these decisions. Use 'averageRating' as your target variable and preprocess your feature set as required by your selected models.\n",
    "  \n",
    "* Implement each of your five chosen regression models using the prepared data. Train each model on the training set and generate predictions for the test set. Create a dataframe that combines the predictions from all five models for the test data, and display the first few rows of this dataframe.\n",
    "  \n",
    "* Evaluate the performance of all five models by calculating the Root Mean Square Error (RMSE) and R-squared values for both the training and testing data. Present these metrics in a clear, tabular format for easy comparison. Based on these results, select the best-performing model and provide a detailed explanation for your choice. Finally, critically assess whether your best model is suitable for production use. Discuss its strengths and limitations, and propose specific strategies or techniques you would employ to further improve the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3.  \n",
    "You will make classification models here. Use Rating as the target variable. Make sure to exclude averageRating(Why?) from the predictor variables.\n",
    "\n",
    "* Select five advanced classification models beyond basic ones like Multinomial Logistic Regression, Decision Tree, Random Forest, K-Nearest Neighbors, and SVM. Determine appropriate test size and validation method, providing a clear rationale for your choices. Split your data into training and testing sets based on these decisions. Use 'Rating' as your target variable. Explain why 'averageRating' should be excluded. Preprocess your feature set as required by your selected models.\n",
    "\n",
    "* Implement each of your five chosen classification models using the prepared data. Train each model on the training set and generate predictions for the test set. Create a table showing the accuracy scores for all five models on both the training and testing data. For your best-performing model, create and display a colored heatmap of the confusion matrix for the test data predictions.\n",
    "\n",
    "* Select one additional metric beyond accuracy for validating your models, explaining why you chose this metric. Evaluate all five models using this additional metric and determine which model performs best based on it. Critically assess whether your best model is suitable for production use, discussing its strengths and limitations. Propose specific strategies or techniques you would employ to further improve the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4. \n",
    "For this problem, you'll create clustering models using techniques other than K-Means clustering.\n",
    "Here's the clustering problem structured into three parts:\n",
    "\n",
    "* Select relevant columns from the cleaned data for clustering, providing a brief explanation for your choices. Determine the appropriate number of clusters (K) you want to use and justify your reasoning. This step should involve careful consideration of the dataset's characteristics and the potential insights you aim to gain from the clustering process.\n",
    "\n",
    "* Implement two clustering models using techniques other than K-Means clustering with the chosen features. Predict cluster assignments for data points using each model. Add these cluster labels to the original dataset from problem 1, creating new columns to represent the cluster assignments from each model.\n",
    "\n",
    "* For each of the two clustering models, identify two new pieces of information or insights about the dataset using the added cluster labels. Analyze how the clusters relate to other variables in the dataset and what patterns or groupings they reveal. Explain your process for deriving these insights and discuss their potential implications for understanding the IMDb dataset. Consider how these insights might be valuable for further analysis or decision-making in the context of movie ratings and characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5. \n",
    "Upload the AirPassenger data. The dataset has two columns: Month representing the time in a specific format (e.g., \"1949-01\" for January 1949) and #Passengers representing the number of passengers for each corresponding month.\n",
    "\n",
    "*  Extract month and year from the 'Month' column. Perform summary statistics on the '#Passengers' column. Visualize the overall distribution of passenger counts using appropriate plots. Create time series plots to visually inspect trends, seasonality, and patterns. Decompose the time series into trend, seasonality, and residual components, and visualize each component separately.\n",
    "\n",
    "* Apply ARIMA, Exponential Smoothing, and Prophet models to the data. Evaluate the accuracy of all three forecasting methods using appropriate metrics such as RMSE, MAE, and MAPE. Create autocorrelation and partial autocorrelation plots to identify lag relationships in the time series data. Implement and visualize a technique like moving averages or differencing to remove trends and highlight patterns in the data.\n",
    "\n",
    "* Implement an anomaly detection method, such as a rolling average with anomaly thresholds. Visualize and analyze the identified anomalies in the context of the time series. Discuss the implications of these anomalies and how they relate to the overall patterns observed in the data. Compare the results of the anomaly detection with the findings from the forecasting models, and explain any discrepancies or correlations between the two analyses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
