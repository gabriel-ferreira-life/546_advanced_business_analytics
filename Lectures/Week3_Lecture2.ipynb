{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More on Machine Learning\n",
    "## Non-Linear Regression\n",
    "## More On Regression: \n",
    "* Gradient Boosting Regression\n",
    "* Principal Component Regression (PCR)\n",
    "* Bayesian Linear Regression ( DIY)\n",
    "## More On Classification\n",
    "* Naive Bayes Classifier\n",
    "* Gradient Boosting Classifier\n",
    "* Linear Discriminant Analysis (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear Regression Overview\n",
    "* Nonlinear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables when the relationship is not linear.\n",
    "* Many real-world relationships are not strictly linear.\n",
    "* Nonlinear regression allows us to capture more complex patterns in the data.\n",
    "* Provides flexibility in modeling various functional forms.\n",
    "* Suitable for problems where linear models fall short."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "## We can combine Polynomial, Logarithmic and others to make the regression nonlinear\n",
    "* The growth of a population over time\n",
    "* The spread of a disease\n",
    "* The relationship between drug dosage and response\n",
    "* The relationship between income and education level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         x1        x2         y\n",
      "0  0.374540  0.031429  0.612247\n",
      "1  0.950714  0.636410  4.979346\n",
      "2  0.731994  0.314356  3.042628\n",
      "3  0.598658  0.508571  3.321972\n",
      "4  0.156019  0.907566  3.561322\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "# Generate random values for x1 and x2\n",
    "n_samples = 100\n",
    "x1 = np.random.rand(n_samples)\n",
    "x2 = np.random.rand(n_samples)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({'x1': x1, 'x2': x2})\n",
    "\n",
    "# Define the relationship for y\n",
    "df['y'] = 2 * df['x1'] + 3 * df['x2'] + 0.5 * df['x1'] * df['x2'] + \\\n",
    "          0.1 * df['x1']**2 + 0.2 * df['x2']**2 + \\\n",
    "          0.5 * np.sqrt(df['x1']) + 0.3 * np.sqrt(df['x2']) + \\\n",
    "          0.2 * np.log(df['x1']) + 0.1 * np.log(df['x2']) + \\\n",
    "          np.random.normal(scale=0.1, size=n_samples)  # Adding some random noise\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.precision', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y</th>\n",
       "      <th>x1x2</th>\n",
       "      <th>x1^2</th>\n",
       "      <th>x2^2</th>\n",
       "      <th>logx1</th>\n",
       "      <th>logx2</th>\n",
       "      <th>sqrtx1</th>\n",
       "      <th>sqrtx2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.40e-01</td>\n",
       "      <td>9.88e-04</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>-3.46</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.64</td>\n",
       "      <td>4.98</td>\n",
       "      <td>0.61</td>\n",
       "      <td>9.04e-01</td>\n",
       "      <td>4.05e-01</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.73</td>\n",
       "      <td>0.31</td>\n",
       "      <td>3.04</td>\n",
       "      <td>0.23</td>\n",
       "      <td>5.36e-01</td>\n",
       "      <td>9.88e-02</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>-1.16</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.51</td>\n",
       "      <td>3.32</td>\n",
       "      <td>0.30</td>\n",
       "      <td>3.58e-01</td>\n",
       "      <td>2.59e-01</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.16</td>\n",
       "      <td>0.91</td>\n",
       "      <td>3.56</td>\n",
       "      <td>0.14</td>\n",
       "      <td>2.43e-02</td>\n",
       "      <td>8.24e-01</td>\n",
       "      <td>-1.86</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.49</td>\n",
       "      <td>0.35</td>\n",
       "      <td>2.39</td>\n",
       "      <td>0.17</td>\n",
       "      <td>2.44e-01</td>\n",
       "      <td>1.22e-01</td>\n",
       "      <td>-0.71</td>\n",
       "      <td>-1.05</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.52</td>\n",
       "      <td>0.73</td>\n",
       "      <td>4.18</td>\n",
       "      <td>0.38</td>\n",
       "      <td>2.73e-01</td>\n",
       "      <td>5.27e-01</td>\n",
       "      <td>-0.65</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.90</td>\n",
       "      <td>4.39</td>\n",
       "      <td>0.38</td>\n",
       "      <td>1.83e-01</td>\n",
       "      <td>8.05e-01</td>\n",
       "      <td>-0.85</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.89</td>\n",
       "      <td>2.37</td>\n",
       "      <td>0.02</td>\n",
       "      <td>6.46e-04</td>\n",
       "      <td>7.87e-01</td>\n",
       "      <td>-3.67</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.11</td>\n",
       "      <td>0.78</td>\n",
       "      <td>2.77</td>\n",
       "      <td>0.08</td>\n",
       "      <td>1.16e-02</td>\n",
       "      <td>6.08e-01</td>\n",
       "      <td>-2.23</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      x1    x2     y  x1x2      x1^2      x2^2  logx1  logx2  sqrtx1  sqrtx2\n",
       "0   0.37  0.03  0.61  0.01  1.40e-01  9.88e-04  -0.98  -3.46    0.61    0.18\n",
       "1   0.95  0.64  4.98  0.61  9.04e-01  4.05e-01  -0.05  -0.45    0.98    0.80\n",
       "2   0.73  0.31  3.04  0.23  5.36e-01  9.88e-02  -0.31  -1.16    0.86    0.56\n",
       "3   0.60  0.51  3.32  0.30  3.58e-01  2.59e-01  -0.51  -0.68    0.77    0.71\n",
       "4   0.16  0.91  3.56  0.14  2.43e-02  8.24e-01  -1.86  -0.10    0.39    0.95\n",
       "..   ...   ...   ...   ...       ...       ...    ...    ...     ...     ...\n",
       "95  0.49  0.35  2.39  0.17  2.44e-01  1.22e-01  -0.71  -1.05    0.70    0.59\n",
       "96  0.52  0.73  4.18  0.38  2.73e-01  5.27e-01  -0.65  -0.32    0.72    0.85\n",
       "97  0.43  0.90  4.39  0.38  1.83e-01  8.05e-01  -0.85  -0.11    0.65    0.95\n",
       "98  0.03  0.89  2.37  0.02  6.46e-04  7.87e-01  -3.67  -0.12    0.16    0.94\n",
       "99  0.11  0.78  2.77  0.08  1.16e-02  6.08e-01  -2.23  -0.25    0.33    0.88\n",
       "\n",
       "[100 rows x 10 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding new columns\n",
    "df['x1x2'] = df['x1'] * df['x2']\n",
    "df['x1^2'] = df['x1']**2\n",
    "df['x2^2'] = df['x2']**2\n",
    "df['logx1'] = np.log(df['x1'])\n",
    "df['logx2'] = np.log(df['x2'])\n",
    "df['sqrtx1'] = np.sqrt(df['x1'])\n",
    "df['sqrtx2'] = np.sqrt(df['x2'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression (x1 and x2):\n",
      "R^2 Score: 0.9899044661254759\n",
      "\n",
      "Linear Regression (all columns):\n",
      "R^2 Score: 0.9960396838098363\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Linear regression using only x1 and x2\n",
    "X1X2 = df[['x1', 'x2']]\n",
    "y = df['y']\n",
    "\n",
    "model_x1x2 = LinearRegression()\n",
    "model_x1x2.fit(X1X2, y)\n",
    "y_pred_x1x2 = model_x1x2.predict(X1X2)\n",
    "r2_x1x2 = r2_score(y, y_pred_x1x2)\n",
    "\n",
    "print(\"Linear Regression (x1 and x2):\")\n",
    "print(\"R^2 Score:\", r2_x1x2)\n",
    "\n",
    "# Linear regression using all columns\n",
    "X_all = df.drop('y', axis=1)\n",
    "model_all = LinearRegression()\n",
    "model_all.fit(X_all, y)\n",
    "y_pred_all = model_all.predict(X_all)\n",
    "r2_all = r2_score(y, y_pred_all)\n",
    "\n",
    "print(\"\\nLinear Regression (all columns):\")\n",
    "print(\"R^2 Score:\", r2_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polynomial Regression (x1 and x2):\n",
      "R^2 Score: 0.993139617017216\n",
      "\n",
      "Polynomial Regression (all columns):\n",
      "R^2 Score: 0.9972279034747895\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import r2_score\n",
    "X1X2 = df[['x1', 'x2']]\n",
    "y = df['y']\n",
    "\n",
    "degree = 2  # You can adjust the degree as needed\n",
    "\n",
    "poly_model_x1x2 = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "poly_model_x1x2.fit(X1X2, y)\n",
    "y_pred_poly_x1x2 = poly_model_x1x2.predict(X1X2)\n",
    "r2_poly_x1x2 = r2_score(y, y_pred_poly_x1x2)\n",
    "\n",
    "print(\"Polynomial Regression (x1 and x2):\")\n",
    "print(\"R^2 Score:\", r2_poly_x1x2)\n",
    "\n",
    "# Polynomial regression using all columns\n",
    "X_all = df.drop('y', axis=1)\n",
    "\n",
    "poly_model_all = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "poly_model_all.fit(X_all, y)\n",
    "y_pred_poly_all = poly_model_all.predict(X_all)\n",
    "r2_poly_all = r2_score(y, y_pred_poly_all)\n",
    "\n",
    "print(\"\\nPolynomial Regression (all columns):\")\n",
    "print(\"R^2 Score:\", r2_poly_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.33</td>\n",
       "      <td>452600.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.30</td>\n",
       "      <td>358500.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88                41.0        880.0           129.0   \n",
       "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
       "\n",
       "   population  households  median_income  median_house_value ocean_proximity  \n",
       "0       322.0       126.0           8.33            452600.0        NEAR BAY  \n",
       "1      2401.0      1138.0           8.30            358500.0        NEAR BAY  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.read_csv('housing.csv')\n",
    "df = df.dropna()\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1H OCEAN     9034\n",
       "INLAND        6496\n",
       "NEAR OCEAN    2628\n",
       "NEAR BAY      2270\n",
       "ISLAND           5\n",
       "Name: ocean_proximity, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ocean_proximity.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting:\n",
    "\n",
    "* Start with a Simple Model: Begin with a basic model (e.g., decision tree).\n",
    "* Initial Prediction: Make predictions with the simple model.\n",
    "* Calculate Residuals: Find the differences between actual and predicted values.\n",
    "* Build a New Model: Create a new model to predict residuals.\n",
    "* Update Prediction: Add the new model's output to the previous prediction.\n",
    "* Repeat: Iteratively build models to correct errors and refine predictions.\n",
    "* Final Prediction: Combine predictions for a powerful ensemble model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Boosting in a Nutshell:**\n",
    "\n",
    "1. **Objective:**\n",
    "   - Minimize the square loss by building a model $F(x)$.\n",
    "\n",
    "2. **Given:**\n",
    "   - Dataset $((x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n))$\n",
    "\n",
    "3. **Improving Model $F$:**\n",
    "   - Residuals: $y_i - F(x_i)$ represent model deficiencies.\n",
    "   - Goal: Enhance $F$ with an additional model $h$ to compensate for residuals.\n",
    "\n",
    "4. **Gradient Boosting Iteration:**\n",
    "   - Build $h_1$ to compensate $F$ deficiencies: $F_1(x) = F(x) + h_1(x)$.\n",
    "   - If $F_1$ is not perfect, build $h_2$: $F_2(x) = F_1(x) + h_2(x)$.\n",
    "   - Repeat until satisfied.\n",
    "\n",
    "5. **Test Data Generalization:**\n",
    "   - Improvements on training data extend to new, unseen data.\n",
    "\n",
    "6. **Connection to Gradient Descent:**\n",
    "   - Residuals act like \"errors\" in the current model.\n",
    "   - Analogous to gradient descent, each $h$ is chosen to move towards the steepest descent of the loss function.\n",
    "\n",
    "**Formula:**\n",
    "$$ F(x) = \\sum_{m=1}^M \\gamma_m h_m(x) $$\n",
    "\n",
    "**Key Concepts:**\n",
    "- $M$: Number of weak learners (trees).\n",
    "- $\\gamma_m$: Learning rate for the $m$-th tree.\n",
    "- $h_m(x)$: Weak learner (tree) at iteration $m$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop a categorical variable now.\n",
    "df = df.drop('ocean_proximity', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting for  Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 3087399399.5374117\n",
      "R-squared Score: 0.7742333759355379\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "X = df.drop('median_house_value', axis=1)\n",
    "y = df['median_house_value']\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Building the Gradient Boosting model\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = gb_model.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R-squared Score: {r2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     20433.00\n",
       "mean     206864.41\n",
       "std      115435.67\n",
       "min       14999.00\n",
       "25%      119500.00\n",
       "50%      179700.00\n",
       "75%      264700.00\n",
       "max      500001.00\n",
       "Name: median_house_value, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Very Low    5110\n",
       "Moderate    5109\n",
       "Low         5107\n",
       "High        5107\n",
       "Name: median_price_category, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['median_price_category'] = pd.cut(df['median_house_value'],\n",
    "                                     bins=[-float('inf'), 119500, 179700, 264700, float('inf')],\n",
    "                                     labels=['Very Low', 'Low', 'Moderate', 'High'],\n",
    "                                     include_lowest=True)\n",
    "\n",
    "df['median_price_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7093\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        High       0.84      0.76      0.79      1062\n",
      "         Low       0.65      0.60      0.63      1054\n",
      "    Moderate       0.59      0.65      0.62       960\n",
      "    Very Low       0.77      0.82      0.79      1011\n",
      "\n",
      "    accuracy                           0.71      4087\n",
      "   macro avg       0.71      0.71      0.71      4087\n",
      "weighted avg       0.71      0.71      0.71      4087\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "X = df.drop(['median_house_value', 'median_price_category'], axis=1)\n",
    "\n",
    "# Target variable\n",
    "y = df['median_price_category']\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Gradient Boosting Classifier\n",
    "classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_report_str = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print('\\nClassification Report:\\n', classification_report_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Linear Regression\n",
    "\n",
    "## Objective\n",
    "- Apply Bayesian statistics to linear regression for uncertainty estimates in predictions.\n",
    "\n",
    "## Given\n",
    "- Dataset $(X, y)$ with input features $X$ and target variable $y$.\n",
    "\n",
    "## Model\n",
    "- Linear regression model: $y = X\\beta + \\epsilon$\n",
    "- Assume a prior distribution for coefficients $\\beta$ and noise $\\epsilon$.\n",
    "\n",
    "## Bayesian Formulation\n",
    "- Posterior distribution: $P(\\beta, \\sigma^2 | X, y) \\propto P(y | X, \\beta, \\sigma^2) \\cdot P(\\beta) \\cdot P(\\sigma^2)$\n",
    "\n",
    "The left-hand side of the equation, $P(\\beta, \\sigma^2 | X, y)$, represents the posterior distribution. The right-hand side of the equation is the product of three terms:\n",
    "\n",
    "- $P(y | X, \\beta, \\sigma^2)$ is the likelihood function. It represents the probability of observing the data given the values of the parameters.\n",
    "\n",
    "- $P(\\beta)$ is the prior distribution for $\\beta$. It represents our beliefs about the values of $\\beta$ before we have seen the data.\n",
    "\n",
    "- $P(\\sigma^2)$ is the prior distribution for $\\sigma^2$. It represents our beliefs about the values of $\\sigma^2$ before we have seen the data.\n",
    "\n",
    "## Key Equations\n",
    "1. **Likelihood:** $P(y | X, \\beta, \\sigma^2) \\sim \\mathcal{N}(X\\beta, \\sigma^2 I)$\n",
    "   - Explanation: This represents the likelihood of observing the target variable $ y $ given the input features $ X $, coefficients $ \\beta $, and noise variance $ \\sigma^2 $. It assumes a normal (Gaussian) distribution with mean $ X\\beta $ and variance $ \\sigma^2 I $, where $ I $ is the identity matrix.\n",
    "2. **Prior:** $P(\\beta) \\sim \\mathcal{N}(0, \\Sigma)$\n",
    "- Explanation: This is the prior distribution for the coefficients $ \\beta $. It assumes a normal distribution with mean 0 and covariance matrix $ \\Sigma $. The prior represents our beliefs or knowledge about the likely values of the coefficients before observing the data.\n",
    "3. **Prior for Noise:** $P(\\sigma^2) \\sim \\text{Inv-Gamma}(\\alpha, \\beta)$\n",
    "- Explanation: This is the prior distribution for the noise variance $ \\sigma^2 $. It assumes an inverse-gamma distribution with shape parameter $ \\alpha $ and scale parameter $ \\beta $. The prior captures our uncertainty about the amount of noise in the data.\n",
    "4. **Posterior:** $P(\\beta, \\sigma^2 | X, y) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2} \\|y - X\\beta\\|^2 - \\frac{1}{2} \\beta^T \\Sigma^{-1} \\beta - \\frac{\\alpha + n}{2} \\log(\\beta)\\right)$\n",
    "- Explanation: This is the posterior distribution, representing our updated beliefs about the coefficients and noise variance after observing the data. The terms in the exponent are derived from the product of the likelihood, prior, and prior for noise. The posterior is proportional to the product of these three distributions.\n",
    "\n",
    "## Inference\n",
    "- Use the posterior distribution to make predictions and estimate uncertainties.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This needs pymc3 package.\n",
    "### https://www.pymc.io/projects/docs/en/stable/learn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymc3 in /Users/mann/anaconda3/lib/python3.10/site-packages (3.11.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /Users/mann/anaconda3/lib/python3.10/site-packages (from pymc3) (4.8.0)\n",
      "Requirement already satisfied: patsy>=0.5.1 in /Users/mann/anaconda3/lib/python3.10/site-packages (from pymc3) (0.5.3)\n",
      "Requirement already satisfied: pandas>=0.24.0 in /Users/mann/anaconda3/lib/python3.10/site-packages (from pymc3) (1.5.3)\n",
      "Requirement already satisfied: deprecat in /Users/mann/anaconda3/lib/python3.10/site-packages (from pymc3) (2.1.1)\n",
      "Requirement already satisfied: cachetools>=4.2.1 in /Users/mann/anaconda3/lib/python3.10/site-packages (from pymc3) (5.3.2)\n",
      "Requirement already satisfied: fastprogress>=0.2.0 in /Users/mann/anaconda3/lib/python3.10/site-packages (from pymc3) (1.0.3)\n",
      "Collecting scipy<1.8.0,>=1.7.3\n",
      "  Using cached scipy-1.7.3-1-cp310-cp310-macosx_12_0_arm64.whl (27.0 MB)\n",
      "Requirement already satisfied: semver>=2.13.0 in /Users/mann/anaconda3/lib/python3.10/site-packages (from pymc3) (3.0.2)\n",
      "Requirement already satisfied: arviz>=0.11.0 in /Users/mann/anaconda3/lib/python3.10/site-packages (from pymc3) (0.12.1)\n",
      "Collecting numpy<1.22.2,>=1.15.0\n",
      "  Using cached numpy-1.22.1-cp310-cp310-macosx_11_0_arm64.whl (12.8 MB)\n",
      "Requirement already satisfied: dill in /Users/mann/anaconda3/lib/python3.10/site-packages (from pymc3) (0.3.6)\n",
      "Requirement already satisfied: theano-pymc==1.1.2 in /Users/mann/anaconda3/lib/python3.10/site-packages (from pymc3) (1.1.2)\n",
      "Requirement already satisfied: filelock in /Users/mann/anaconda3/lib/python3.10/site-packages (from theano-pymc==1.1.2->pymc3) (3.9.0)\n",
      "Requirement already satisfied: xarray-einstats>=0.2 in /Users/mann/anaconda3/lib/python3.10/site-packages (from arviz>=0.11.0->pymc3) (0.6.0)\n",
      "Requirement already satisfied: matplotlib>=3.0 in /Users/mann/anaconda3/lib/python3.10/site-packages (from arviz>=0.11.0->pymc3) (3.7.0)\n",
      "Requirement already satisfied: setuptools>=38.4 in /Users/mann/anaconda3/lib/python3.10/site-packages (from arviz>=0.11.0->pymc3) (65.6.3)\n",
      "Requirement already satisfied: netcdf4 in /Users/mann/anaconda3/lib/python3.10/site-packages (from arviz>=0.11.0->pymc3) (1.6.5)\n",
      "Requirement already satisfied: xarray>=0.16.1 in /Users/mann/anaconda3/lib/python3.10/site-packages (from arviz>=0.11.0->pymc3) (2022.11.0)\n",
      "Requirement already satisfied: packaging in /Users/mann/anaconda3/lib/python3.10/site-packages (from arviz>=0.11.0->pymc3) (22.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/mann/anaconda3/lib/python3.10/site-packages (from pandas>=0.24.0->pymc3) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/mann/anaconda3/lib/python3.10/site-packages (from pandas>=0.24.0->pymc3) (2022.7)\n",
      "Requirement already satisfied: six in /Users/mann/anaconda3/lib/python3.10/site-packages (from patsy>=0.5.1->pymc3) (1.16.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /Users/mann/anaconda3/lib/python3.10/site-packages (from deprecat->pymc3) (1.14.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/mann/anaconda3/lib/python3.10/site-packages (from matplotlib>=3.0->arviz>=0.11.0->pymc3) (3.0.9)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/mann/anaconda3/lib/python3.10/site-packages (from matplotlib>=3.0->arviz>=0.11.0->pymc3) (1.0.5)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/mann/anaconda3/lib/python3.10/site-packages (from matplotlib>=3.0->arviz>=0.11.0->pymc3) (1.4.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/mann/anaconda3/lib/python3.10/site-packages (from matplotlib>=3.0->arviz>=0.11.0->pymc3) (4.25.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/mann/anaconda3/lib/python3.10/site-packages (from matplotlib>=3.0->arviz>=0.11.0->pymc3) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/mann/anaconda3/lib/python3.10/site-packages (from matplotlib>=3.0->arviz>=0.11.0->pymc3) (9.4.0)\n",
      "Requirement already satisfied: certifi in /Users/mann/anaconda3/lib/python3.10/site-packages (from netcdf4->arviz>=0.11.0->pymc3) (2022.12.7)\n",
      "Requirement already satisfied: cftime in /Users/mann/anaconda3/lib/python3.10/site-packages (from netcdf4->arviz>=0.11.0->pymc3) (1.6.3)\n",
      "Installing collected packages: numpy, scipy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.3\n",
      "    Uninstalling numpy-1.26.3:\n",
      "      Successfully uninstalled numpy-1.26.3\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.12.0\n",
      "    Uninstalling scipy-1.12.0:\n",
      "      Successfully uninstalled scipy-1.12.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gensim 4.3.0 requires FuzzyTM>=0.4.0, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.22.1 scipy-1.7.3\n"
     ]
    }
   ],
   "source": [
    "!pip install pymc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Naive Bayes Classifier\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Naive Bayes is a probabilistic classification algorithm based on Bayes' theorem. It assumes independence among features, making it \"naive\" but often effective for text classification and spam filtering.\n",
    "\n",
    "## Bayes' Theorem\n",
    "\n",
    "Bayes' theorem is the foundation of Naive Bayes. It calculates the probability of a hypothesis based on prior knowledge.\n",
    "\n",
    "$$\n",
    "P(A | B) = \\frac{P(B | A) \\cdot P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ P(A | B) $ is the posterior probability.\n",
    "- $ P(B | A) $ is the likelihood.\n",
    "- $ P(A) $ is the prior probability.\n",
    "- $ P(B) $ is the evidence.\n",
    "\n",
    "## Naive Bayes Classification\n",
    "\n",
    "The Naive Bayes classifier assumes independence among features given the class. The probability of a class $ C_k $ given features $ x_1, x_2, \\ldots, x_n $ can be expressed as:\n",
    "\n",
    "$$\n",
    "P(C_k | x_1, x_2, \\ldots, x_n) \\propto P(C_k) \\cdot \\prod_{i=1}^{n} P(x_i | C_k)\n",
    "$$\n",
    "\n",
    "## Types of Naive Bayes Classifiers\n",
    "\n",
    "### 1. Multinomial Naive Bayes\n",
    "\n",
    "- Suitable for discrete data, often used for document classification.\n",
    "\n",
    "### 2. Gaussian Naive Bayes\n",
    "\n",
    "- Assumes features follow a normal distribution.\n",
    "\n",
    "### 3. Bernoulli Naive Bayes\n",
    "\n",
    "- Appropriate for binary features.\n",
    "\n",
    "## Training and Prediction\n",
    "\n",
    "1. **Training:**\n",
    "   - Estimate prior probabilities $ P(C_k) $ and class-conditional probabilities $ P(x_i | C_k) $ from the training data.\n",
    "\n",
    "2. **Prediction:**\n",
    "   - Calculate posterior probabilities using the Naive Bayes formula.\n",
    "   - Assign the class with the highest posterior probability.\n",
    "\n",
    "## Advantages\n",
    "\n",
    "- Simple and computationally efficient.\n",
    "- Performs well on high-dimensional data.\n",
    "\n",
    "## Limitations\n",
    "\n",
    "- Assumes independence, which may not always hold.\n",
    "- Sensitivity to irrelevant features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.49\n",
      "\n",
      "Confusion Matrix:\n",
      "[[541  38 301 182]\n",
      " [ 22 217 264 551]\n",
      " [128 129 409 294]\n",
      " [  4 120  37 850]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.78      0.51      0.62      1062\n",
      "         Low       0.43      0.21      0.28      1054\n",
      "    Moderate       0.40      0.43      0.42       960\n",
      "    Very Low       0.45      0.84      0.59      1011\n",
      "\n",
      "    accuracy                           0.49      4087\n",
      "   macro avg       0.52      0.50      0.47      4087\n",
      "weighted avg       0.52      0.49      0.47      4087\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "X = df.drop(['median_house_value', 'median_price_category'], axis=1)\n",
    "y = df['median_price_category']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and fit the Naive Bayes classifier (Gaussian)\n",
    "naive_bayes = GaussianNB()\n",
    "naive_bayes.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = naive_bayes.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Display results\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print('\\nConfusion Matrix:')\n",
    "print(conf_matrix)\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n",
       "       'total_bedrooms', 'population', 'households', 'median_income'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Regression (PCR)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Principal Component Regression (PCR) is a technique that combines the concepts of Principal Component Analysis (PCA) and linear regression. It aims to handle multicollinearity in regression models by transforming the original features into a new set of uncorrelated variables known as principal components.\n",
    "\n",
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "PCA is a dimensionality reduction technique that transforms the original features into a set of linearly uncorrelated variables called principal components. The first principal component explains the most variance, followed by the second, and so on.\n",
    "\n",
    "### PCA Equation\n",
    "\n",
    "The principal components are obtained by linear combinations of the original features:\n",
    "\n",
    "$$\n",
    "Z_i = \\sum_{j=1}^{p} \\phi_{ij}X_j\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $Z_i$ is the $i$-th principal component,\n",
    "- $\\phi_{ij}$ is the loading of the $j$-th feature on the $i$-th principal component,\n",
    "- $X_j$ is the $j$-th original feature,\n",
    "- $p$ is the number of features.\n",
    "\n",
    "## Principal Component Regression (PCR)\n",
    "\n",
    "PCR involves performing PCA on the original features and then using a subset of the principal components as predictors in a linear regression model.\n",
    "\n",
    "### PCR Equation\n",
    "\n",
    "The regression equation using \\(m\\) principal components is:\n",
    "\n",
    "$$\n",
    "Y = \\beta_0 + \\sum_{i=1}^{m} \\beta_i Z_i\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $Y$ is the dependent variable,\n",
    "- $\\beta_0$ is the intercept term,\n",
    "- $\\beta_i$ are the regression coefficients for the principal components,\n",
    "- $Z_i$ are the selected principal components.\n",
    "\n",
    "## Steps in PCR\n",
    "\n",
    "1. Standardize the original features.\n",
    "2. Perform PCA to obtain principal components.\n",
    "3. Select a subset of principal components based on explained variance or cross-validation.\n",
    "4. Use selected principal components as predictors in a linear regression model.\n",
    "\n",
    "## Advantages\n",
    "\n",
    "- Addresses multicollinearity in regression.\n",
    "- Reduces dimensionality while preserving important information.\n",
    "\n",
    "## Limitations\n",
    "\n",
    "- Interpretability can be challenging.\n",
    "- The choice of the number of principal components requires consideration.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 4921881237.63\n",
      "R-squared: 0.64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "X = df.drop(['median_house_value', 'median_price_category'], axis=1)\n",
    "y = df['median_house_value']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a PCR pipeline\n",
    "pcr_model = make_pipeline(StandardScaler(), PCA(), LinearRegression())\n",
    "\n",
    "# Fit the model on training data\n",
    "pcr_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = pcr_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Display results\n",
    "print(f'Mean Squared Error: {mse:.2f}')\n",
    "print(f'R-squared: {r2:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Discriminant Analysis (LDA)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Linear Discriminant Analysis (LDA) is a dimensionality reduction and classification technique that finds the linear combinations of features that best separate different classes in a dataset.\n",
    "\n",
    "## Objective\n",
    "\n",
    "Find a linear transformation of the features to maximize the separation between classes.\n",
    "\n",
    "## Mathematical Formulation\n",
    "\n",
    "### Within-Class Scatter Matrix (S_W)\n",
    "\n",
    "$$\n",
    "S_W = \\sum_{i=1}^{C} \\sum_{j=1}^{n_i} (x_{ij} - \\mu_i)(x_{ij} - \\mu_i)^T\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $C$ is the number of classes.\n",
    "- $n_i$ is the number of samples in class $i$.\n",
    "- $x_{ij}$ is the $j$-th sample in class $i$.\n",
    "- $\\mu_i$ is the mean vector of class $i$.\n",
    "\n",
    "### Between-Class Scatter Matrix (S_B)\n",
    "\n",
    "$$\n",
    "S_B = \\sum_{i=1}^{C} N_i (\\mu_i - \\mu)(\\mu_i - \\mu)^T\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $N_i$ is the number of samples in class \\(i\\).\n",
    "- $\\mu$ is the overall mean vector.\n",
    "\n",
    "### Eigenvalue Problem\n",
    "\n",
    "Solve the generalized eigenvalue problem:\n",
    "\n",
    "$$\n",
    "S_W^{-1}S_B \\mathbf{v} = \\lambda \\mathbf{v}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{v}$ is the eigenvector.\n",
    "- $\\lambda$ is the eigenvalue.\n",
    "\n",
    "### Fisher's Linear Discriminant\n",
    "\n",
    "The optimal projection vector \\(\\mathbf{w}\\) is the eigenvector corresponding to the largest eigenvalue.\n",
    "\n",
    "$$\n",
    "\\mathbf{w} \\propto S_W^{-1} (\\mu_1 - \\mu_2)\n",
    "$$\n",
    "\n",
    "## Classification\n",
    "\n",
    "Project new samples onto the discriminant vector and assign to the class with the closest mean.\n",
    "\n",
    "$$\n",
    "y = \\text{argmax}_i(\\mathbf{w}^T x_i)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $y$ is the predicted class.\n",
    "- $x_i$ is the new sample.\n",
    "\n",
    "## Advantages\n",
    "\n",
    "- Effective in high-dimensional spaces.\n",
    "- Assumes normality and equality of covariance matrices.\n",
    "\n",
    "## Limitations\n",
    "\n",
    "- Sensitive to outliers.\n",
    "- Assumes features are normally distributed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.61\n",
      "\n",
      "Confusion Matrix:\n",
      "[[690  39 324   9]\n",
      " [ 23 524 309 198]\n",
      " [182 174 559  45]\n",
      " [  3 245  57 706]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.77      0.65      0.70      1062\n",
      "         Low       0.53      0.50      0.51      1054\n",
      "    Moderate       0.45      0.58      0.51       960\n",
      "    Very Low       0.74      0.70      0.72      1011\n",
      "\n",
      "    accuracy                           0.61      4087\n",
      "   macro avg       0.62      0.61      0.61      4087\n",
      "weighted avg       0.62      0.61      0.61      4087\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "X = df.drop(['median_house_value', 'median_price_category'], axis=1)\n",
    "y = df['median_price_category']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and fit the LDA model\n",
    "lda_model = LinearDiscriminantAnalysis()\n",
    "lda_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = lda_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Display results\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print('\\nConfusion Matrix:')\n",
    "print(conf_matrix)\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
